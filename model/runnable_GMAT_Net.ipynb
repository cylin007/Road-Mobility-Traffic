{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "hW17_b4CPlvs"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L1zkMjUgAegY"
      },
      "source": [
        "### GPU info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GN0XPaF2AjzK"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9tGaekNkUp8_"
      },
      "source": [
        "### Drive連結"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qnHuSwLjt8yG"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eGynxt4w_yNB"
      },
      "source": [
        "### 切換路徑"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZFXiA5d01lx6"
      },
      "outputs": [],
      "source": [
        "cd drive/MyDrive/colab_pro_plus/SIGKDD_23"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dPAbmQ4DANlg"
      },
      "source": [
        "## Model Architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pxZpG_QUANlg"
      },
      "source": [
        "### Lib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "42iXEetaANlg"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Util #\n",
        "import pickle\n",
        "import numpy as np\n",
        "import os\n",
        "import scipy.sparse as sp\n",
        "import torch\n",
        "from scipy.sparse import linalg\n",
        "from torch.autograd import Variable\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Layer #\n",
        "from __future__ import division\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import init\n",
        "import numbers\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Model #\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "import sys\n",
        "\n",
        "# Trainer #\n",
        "import torch.optim as optim\n",
        "import math\n",
        "\n",
        "# Main #\n",
        "import torch\n",
        "import numpy as np\n",
        "import argparse\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "from torch.utils.tensorboard import SummaryWriter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9m9BJMEvANlh"
      },
      "source": [
        "### Util"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v3lArIYWANlh"
      },
      "outputs": [],
      "source": [
        "\n",
        "class DataLoaderM(object):\n",
        "    def __init__(self, xs, ys,xs_f1,xs_f2,xs_f3,xs_f4, batch_size, pad_with_last_sample=True):\n",
        "        \"\"\"\n",
        "        :param xs:\n",
        "        :param ys:\n",
        "        :param batch_size:\n",
        "        :param pad_with_last_sample: pad with the last sample to make number of samples divisible to batch_size.\n",
        "        \"\"\"\n",
        "        self.batch_size = batch_size\n",
        "        self.current_ind = 0\n",
        "\n",
        "        # 將資料長度補齊至batch_size可整除之數量\n",
        "        # 補齊方法: 取原資料最後一個並複製多個來補齊\n",
        "        if pad_with_last_sample:\n",
        "            # 計算需補齊數量\n",
        "            num_padding = (batch_size - (len(xs) % batch_size)) % batch_size\n",
        "            \n",
        "            x_padding = np.repeat(xs[-1:], num_padding, axis=0)\n",
        "            y_padding = np.repeat(ys[-1:], num_padding, axis=0)\n",
        "\n",
        "            # 將複製後的ele進行concatenate以補齊成可整除batch_size之長度\n",
        "            xs = np.concatenate([xs, x_padding], axis=0)\n",
        "            ys = np.concatenate([ys, y_padding], axis=0)\n",
        "            \n",
        "            ### f1 ###\n",
        "            # 計算需補齊數量\n",
        "            x_padding = np.repeat(xs_f1[-1:], num_padding, axis=0)\n",
        "            \n",
        "            # 將複製後的ele進行concatenate以補齊成可整除batch_size之長度\n",
        "            xs_f1 = np.concatenate([xs_f1, x_padding], axis=0)\n",
        "            \n",
        "            ### f2 ###\n",
        "            # 計算需補齊數量\n",
        "            x_padding = np.repeat(xs_f2[-1:], num_padding, axis=0)\n",
        "\n",
        "            # 將複製後的ele進行concatenate以補齊成可整除batch_size之長度\n",
        "            xs_f2 = np.concatenate([xs_f2, x_padding], axis=0)\n",
        "            \n",
        "            \n",
        "            ### f3 ###\n",
        "            # 計算需補齊數量\n",
        "            x_padding = np.repeat(xs_f3[-1:], num_padding, axis=0)\n",
        "            \n",
        "            # 將複製後的ele進行concatenate以補齊成可整除batch_size之長度\n",
        "            xs_f3 = np.concatenate([xs_f3, x_padding], axis=0)\n",
        "            \n",
        "            \n",
        "            ### f4 ###\n",
        "            # 計算需補齊數量\n",
        "            x_padding = np.repeat(xs_f4[-1:], num_padding, axis=0)\n",
        "\n",
        "            # 將複製後的ele進行concatenate以補齊成可整除batch_size之長度\n",
        "            xs_f4 = np.concatenate([xs_f4, x_padding], axis=0)\n",
        "            \n",
        "            \n",
        "        self.size = len(xs)\n",
        "        self.num_batch = int(self.size // self.batch_size)\n",
        "        self.xs = xs\n",
        "        self.ys = ys\n",
        "        \n",
        "        self.xs_f1 = xs_f1\n",
        "        \n",
        "        self.xs_f2 = xs_f2\n",
        "        \n",
        "        self.xs_f4 = xs_f4\n",
        "        \n",
        "        self.xs_f3 = xs_f3\n",
        "\n",
        "    def shuffle(self):\n",
        "        permutation = np.random.permutation(self.size)\n",
        "        xs, ys = self.xs[permutation], self.ys[permutation]\n",
        "        xs_f1 = self.xs_f1[permutation] \n",
        "        xs_f2 = self.xs_f2[permutation] \n",
        "        \n",
        "        \n",
        "        xs_f3 = self.xs_f3[permutation] \n",
        "        xs_f4 = self.xs_f4[permutation] \n",
        "        \n",
        "        self.xs = xs\n",
        "        self.ys = ys\n",
        "        \n",
        "        self.xs_f1 = xs_f1\n",
        "        \n",
        "        self.xs_f2 = xs_f2\n",
        "        \n",
        "        self.xs_f4 = xs_f4\n",
        "        \n",
        "        self.xs_f3 = xs_f3\n",
        "\n",
        "    def get_iterator(self):\n",
        "        self.current_ind = 0\n",
        "        def _wrapper():\n",
        "            while self.current_ind < self.num_batch:\n",
        "                start_ind = self.batch_size * self.current_ind\n",
        "                end_ind = min(self.size, self.batch_size * (self.current_ind + 1))\n",
        "                x_i = self.xs[start_ind: end_ind, ...]\n",
        "                y_i = self.ys[start_ind: end_ind, ...]\n",
        "                \n",
        "                x_i_f1 = self.xs_f1[start_ind: end_ind, ...]\n",
        "                \n",
        "                x_i_f2 = self.xs_f2[start_ind: end_ind, ...]\n",
        "                \n",
        "                x_i_f4 = self.xs_f4[start_ind: end_ind, ...]\n",
        "                \n",
        "                x_i_f3 = self.xs_f3[start_ind: end_ind, ...]\n",
        "                \n",
        "                # 節省記憶體:\n",
        "                # yield 設計來的目的，就是為了單次輸出內容\n",
        "                # 我們可以把 yield 暫時看成 return，但是這個 return 的功能只有單次\n",
        "                # 而且，一旦我們的程式執行到 yield 後，程式就會把值丟出，並暫時停止\n",
        "                yield (x_i, y_i,x_i_f1,x_i_f2, x_i_f3, x_i_f4)\n",
        "                self.current_ind += 1\n",
        "\n",
        "        return _wrapper()\n",
        "\n",
        "class StandardScaler():\n",
        "    \"\"\"\n",
        "    Standard the input\n",
        "    \"\"\"\n",
        "    def __init__(self, mean, std):\n",
        "        self.mean = mean\n",
        "        self.std = std\n",
        "    def transform(self, data):\n",
        "        return (data - self.mean) / self.std\n",
        "    def inverse_transform(self, data):\n",
        "        return (data * self.std) + self.mean\n",
        "\n",
        "def asym_adj(adj):\n",
        "    \"\"\"Asymmetrically normalize adjacency matrix.\"\"\"\n",
        "    adj = sp.coo_matrix(adj)\n",
        "    rowsum = np.array(adj.sum(1)).flatten()\n",
        "    d_inv = np.power(rowsum, -1).flatten()\n",
        "    d_inv[np.isinf(d_inv)] = 0.\n",
        "    d_mat= sp.diags(d_inv)\n",
        "    return d_mat.dot(adj).astype(np.float32).todense()\n",
        "\n",
        "\n",
        "def load_pickle(pickle_file):\n",
        "    try:\n",
        "        with open(pickle_file, 'rb') as f:\n",
        "            pickle_data = pickle.load(f)\n",
        "    except UnicodeDecodeError as e:\n",
        "        with open(pickle_file, 'rb') as f:\n",
        "            pickle_data = pickle.load(f, encoding='latin1')\n",
        "    except Exception as e:\n",
        "        print('Unable to load data ', pickle_file, ':', e)\n",
        "        raise\n",
        "    return pickle_data\n",
        "\n",
        "def load_adj(pkl_filename, adjtype):\n",
        "    sensor_ids, sensor_id_to_ind, adj_mx = load_pickle(pkl_filename)\n",
        "\n",
        "    print('# 全部L.A.的sensor ID(sensor_ids):\\n',sensor_ids)\n",
        "    print('# 將sensor ID對應index(sensor_id_to_ind):\\n',sensor_id_to_ind)\n",
        "    \n",
        "    if adjtype == \"doubletransition\":\n",
        "        adj = [asym_adj(adj_mx), asym_adj(np.transpose(adj_mx))]   # asym_adj(adj_mx): forward transition matrix / asym_adj(np.transpose(adj_mx)): backward transition matrix\n",
        "    elif adjtype == \"identity\":\n",
        "        adj = [np.diag(np.ones(adj_mx.shape[0])).astype(np.float32)]\n",
        "    else:\n",
        "        error = 0\n",
        "        assert error, \"adj type not defined\"\n",
        "\n",
        "    print('# Double transition Transition matrix of Eq 4:\\n',adj)\n",
        "    return sensor_ids, sensor_id_to_ind, adj\n",
        "\n",
        "def load_dataset(dataset_dir, batch_size, valid_batch_size= None, test_batch_size=None):\n",
        "    data = {}\n",
        "    for category in ['train', 'val', 'test']:\n",
        "        cat_data = np.load(os.path.join(dataset_dir, category + '.npz'))\n",
        "        data['x_' + category] = cat_data['x']\n",
        "        data['y_' + category] = cat_data['y']\n",
        "\n",
        "        if args.log_print:\n",
        "            print(\"# category:\", category)\n",
        "            print('x:',data['x_' + category].shape, data['x_' + category][0] )\n",
        "            print('y:',data['y_' + category].shape, data['y_' + category][0] )\n",
        "    \n",
        "    # 使用train的mean/std來正規化valid/test #\n",
        "    scaler = StandardScaler(mean=data['x_train'][..., 0].mean(), std=data['x_train'][..., 0].std())\n",
        "    # 將欲訓練特徵改成正規化\n",
        "    for category in ['train', 'val', 'test']:\n",
        "        data['x_' + category][..., 0] = scaler.transform(data['x_' + category][..., 0])\n",
        "\n",
        "    \n",
        "\n",
        "    data['train_loader'] = DataLoaderM(data['x_train'], data['y_train'], batch_size)\n",
        "    data['val_loader'] = DataLoaderM(data['x_val'], data['y_val'], valid_batch_size)\n",
        "    data['test_loader'] = DataLoaderM(data['x_test'], data['y_test'], test_batch_size)\n",
        "    data['scaler'] = scaler\n",
        "    return data\n",
        "\n",
        "\n",
        "\n",
        "def masked_mse(preds, labels, null_val=np.nan):\n",
        "    if np.isnan(null_val):\n",
        "        mask = ~torch.isnan(labels)\n",
        "    else:\n",
        "        mask = (labels!=null_val)\n",
        "    mask = mask.float()\n",
        "    mask /= torch.mean((mask))\n",
        "    mask = torch.where(torch.isnan(mask), torch.zeros_like(mask), mask)\n",
        "    loss = (preds-labels)**2\n",
        "    loss = loss * mask\n",
        "    loss = torch.where(torch.isnan(loss), torch.zeros_like(loss), loss)\n",
        "    return torch.mean(loss)\n",
        "\n",
        "def masked_rmse(preds, labels, null_val=np.nan):\n",
        "    return torch.sqrt(masked_mse(preds=preds, labels=labels, null_val=null_val))\n",
        "\n",
        "\n",
        "def masked_mae(preds, labels, null_val=np.nan):\n",
        "    if np.isnan(null_val):\n",
        "        mask = ~torch.isnan(labels)\n",
        "    else:\n",
        "        mask = (labels!=null_val)\n",
        "    mask = mask.float()\n",
        "    mask /=  torch.mean((mask))\n",
        "    mask = torch.where(torch.isnan(mask), torch.zeros_like(mask), mask)\n",
        "    loss = torch.abs(preds-labels)\n",
        "    loss = loss * mask\n",
        "    loss = torch.where(torch.isnan(loss), torch.zeros_like(loss), loss)\n",
        "    return torch.mean(loss)\n",
        "def masked_mape(preds, labels, null_val=np.nan):\n",
        "    if np.isnan(null_val):\n",
        "        mask = ~torch.isnan(labels)\n",
        "    else:\n",
        "        mask = (labels!=null_val)\n",
        "    mask = mask.float()\n",
        "    mask /=  torch.mean((mask))\n",
        "    mask = torch.where(torch.isnan(mask), torch.zeros_like(mask), mask)\n",
        "    loss = torch.abs(preds-labels)/labels\n",
        "    #loss = 2.0 * torch.mean(torch.abs(preds - labels) / (torch.abs(preds) + torch.abs(labels)))\n",
        "    loss = loss * mask\n",
        "    loss = torch.where(torch.isnan(loss), torch.zeros_like(loss), loss)\n",
        "    return torch.mean(loss)\n",
        "def masked_smape(preds, labels, null_val=np.nan):\n",
        "    if np.isnan(null_val):\n",
        "        mask = ~torch.isnan(labels)\n",
        "    else:\n",
        "        mask = (labels!=null_val)\n",
        "    mask = mask.float()\n",
        "    mask /=  torch.mean((mask))\n",
        "    mask = torch.where(torch.isnan(mask), torch.zeros_like(mask), mask)\n",
        "    #loss = torch.abs(preds-labels)/labels\n",
        "    loss = 2.0 * (torch.abs(preds - labels) / (torch.abs(preds) + torch.abs(labels)))\n",
        "    loss = loss * mask\n",
        "    loss = torch.where(torch.isnan(loss), torch.zeros_like(loss), loss)\n",
        "    return torch.mean(loss)\n",
        "\n",
        "def metric(pred, real):\n",
        "    mae = masked_mae(pred,real,0.0).item()\n",
        "    mape = masked_mape(pred,real,0.0).item()\n",
        "    rmse = masked_rmse(pred,real,0.0).item()\n",
        "    smape = masked_smape(pred,real,0.0).item()\n",
        "    return mae,mape,rmse,smape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r4fdCQ3TANli"
      },
      "source": [
        "### Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X8DYHAPCANli"
      },
      "outputs": [],
      "source": [
        "\n",
        "class linear(nn.Module):\n",
        "    def __init__(self,c_in,c_out,bias=True):\n",
        "        super(linear,self).__init__()\n",
        "        self.mlp = torch.nn.Conv2d(c_in, c_out, kernel_size=(1, 1), padding=(0,0), stride=(1,1), bias=bias)\n",
        "\n",
        "    def forward(self,x):\n",
        "        return self.mlp(x)\n",
        "\n",
        "\n",
        "\n",
        "class LayerNorm(nn.Module):\n",
        "    __constants__ = ['normalized_shape', 'weight', 'bias', 'eps', 'elementwise_affine']\n",
        "    def __init__(self, normalized_shape, eps=1e-5, elementwise_affine=True):\n",
        "        super(LayerNorm, self).__init__()\n",
        "        if isinstance(normalized_shape, numbers.Integral):\n",
        "            normalized_shape = (normalized_shape,)\n",
        "        self.normalized_shape = tuple(normalized_shape)\n",
        "        self.eps = eps\n",
        "        self.elementwise_affine = elementwise_affine\n",
        "        if self.elementwise_affine:\n",
        "            self.weight = nn.Parameter(torch.Tensor(*normalized_shape))\n",
        "            self.bias = nn.Parameter(torch.Tensor(*normalized_shape))\n",
        "        else:\n",
        "            self.register_parameter('weight', None)\n",
        "            self.register_parameter('bias', None)\n",
        "        self.reset_parameters()\n",
        "\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        if self.elementwise_affine:\n",
        "            init.ones_(self.weight)\n",
        "            init.zeros_(self.bias)\n",
        "\n",
        "    def forward(self, input, idx):\n",
        "        if self.elementwise_affine:\n",
        "            return F.layer_norm(input, tuple(input.shape[1:]), self.weight[:,idx,:], self.bias[:,idx,:], self.eps)\n",
        "        else:\n",
        "            return F.layer_norm(input, tuple(input.shape[1:]), self.weight, self.bias, self.eps)\n",
        "\n",
        "    def extra_repr(self):\n",
        "        return '{normalized_shape}, eps={eps}, ' \\\n",
        "            'elementwise_affine={elementwise_affine}'.format(**self.__dict__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trxSH8lL3Xfm"
      },
      "source": [
        "### S-GMAT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CCGdl79u3ZUg"
      },
      "outputs": [],
      "source": [
        "\n",
        "class S_GMAT_base(nn.Module):\n",
        "    def __init__(self, n_heads, in_channel, num_nodes, dropout, bias=True):\n",
        "        super(S_GMAT_base, self).__init__()\n",
        "\n",
        "        self.n_head = n_heads\n",
        "        self.f_in = num_nodes\n",
        "        self.a_src = nn.Parameter(torch.Tensor(self.n_head, num_nodes, 1))\n",
        "        self.a_dst = nn.Parameter(torch.Tensor(self.n_head, num_nodes, 1))\n",
        "\n",
        "        self.leaky_relu = nn.LeakyReLU(negative_slope=0.2)\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "        nn.init.xavier_uniform_(self.a_src, gain=1.414)\n",
        "        nn.init.xavier_uniform_(self.a_dst, gain=1.414)\n",
        "        \n",
        "    def forward(self, h, adj, s_attn):\n",
        "\n",
        "        bs, ch, n, dim = h.size()\n",
        "        \n",
        "        attn_src = torch.matmul(h, self.a_src)\n",
        "        attn_dst = torch.matmul(h, self.a_dst)\n",
        "        attn = attn_src.expand(-1, -1, -1, n) + attn_dst.expand(-1, -1, -1, n).permute(\n",
        "            0, 1, 3, 2\n",
        "        )\n",
        "        attn = self.leaky_relu(attn)\n",
        "        \n",
        "        zero_vec = -9e15*torch.ones_like(attn)\n",
        "        attn = torch.where(adj > 0, attn, zero_vec) \n",
        "        attn = self.softmax(attn) # bs x n_head x n x n\n",
        "\n",
        "        attn = attn + s_attn\n",
        "\n",
        "        alpha = 0.05\n",
        "        all = [h]\n",
        "        h_prime = h\n",
        "        h_prime = alpha*h+ (1-alpha)* torch.matmul(attn, h_prime)\n",
        "        all.append(h_prime)\n",
        "        h_prime = alpha*h+ (1-alpha)* torch.matmul(attn, h_prime)\n",
        "        all.append(h_prime)\n",
        "\n",
        "        return torch.cat(all, dim=1)\n",
        "class S_GMAT(nn.Module):\n",
        "    def __init__(self, n_heads, in_channel, num_nodes, dropout, alpha):\n",
        "        super(S_GMAT, self).__init__()\n",
        "        \n",
        "        self.dropout = dropout\n",
        "        \n",
        "        self.sgmat_layer = S_GMAT_base(\n",
        "                    n_heads, in_channel, num_nodes, dropout\n",
        "                )\n",
        "\n",
        "    def forward(self, x, adj, s_attn):\n",
        "        bs,ch,n,dim = x.size()\n",
        "\n",
        "        x = self.sgmat_layer(x, adj, s_attn)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class S_GMAT_module(nn.Module):\n",
        "    def __init__(self, depth, temporal_len, n_heads, in_channel, num_nodes, mlp, mlp2, dropout, alpha):\n",
        "        super(S_GMAT_module, self).__init__()\n",
        "        \n",
        "\n",
        "        self.sgmat_net = S_GMAT(n_heads, in_channel, temporal_len, dropout, alpha)\n",
        "        \n",
        "        self.mlp_convs_start_1 = nn.Conv2d(in_channel, n_heads, 1)\n",
        "        self.mlp_convs_start_2 = nn.Conv2d(in_channel, n_heads, 1)\n",
        "\n",
        "        self.mlp_convs_end_1 = nn.Conv2d(n_heads*(1+depth), 32, 1)\n",
        "        self.mlp_convs_end_2 = nn.Conv2d(n_heads*(1+depth), 32, 1)\n",
        "\n",
        "        self.dropout1 = nn.Dropout(0.1)\n",
        "        self.dropout2 = nn.Dropout(0.1)\n",
        "\n",
        "        self.m1 = nn.GroupNorm((1+depth), n_heads*(1+depth))\n",
        "        self.m2 = nn.GroupNorm((1+depth), n_heads*(1+depth))\n",
        "\n",
        "        self.mlp1 = (nn.Conv2d(32,32,(1,1))) \n",
        "        self.mlp2 = (nn.Conv2d(32,32,(1,1))) \n",
        "\n",
        "        self.norm1 = nn.LayerNorm([32, num_nodes, temporal_len])\n",
        "        self.norm2 = nn.LayerNorm([32, num_nodes, temporal_len])\n",
        "\n",
        "    def forward(self,x,adj1,adj2, pearson_attn):\n",
        "        \n",
        "        bs, ch, n, dim = x.size()\n",
        "        \n",
        "        x_input = x.clone()\n",
        "        #Encoder\n",
        "        x_input = self.mlp_convs_start_1(x_input)\n",
        "\n",
        "        # S-GMAT\n",
        "        x_input = F.elu(self.sgmat_net(x_input,adj1, pearson_attn))\n",
        "\n",
        "        #Decoder\n",
        "        x_input = self.mlp_convs_end_1(x_input)\n",
        "        \n",
        "        x_input = (x + self.dropout1(x_input))\n",
        "\n",
        "        x_input = F.elu(self.mlp1(x_input))\n",
        "        x_input1 = self.norm1(x_input)\n",
        "\n",
        "        return x_input"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JMzcO02f2siV"
      },
      "source": [
        "### T-GMAT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I0c_oNiO2sjE"
      },
      "outputs": [],
      "source": [
        "\n",
        "class T_GMAT_base(nn.Module):\n",
        "    def __init__(self, n_heads, in_channel, num_nodes, dropout, bias=True):\n",
        "        super(T_GMAT_base, self).__init__()\n",
        "\n",
        "        self.n_head = n_heads\n",
        "        self.f_in = num_nodes\n",
        "        self.a_src = nn.Parameter(torch.Tensor(self.n_head, num_nodes, 1))\n",
        "        self.a_dst = nn.Parameter(torch.Tensor(self.n_head, num_nodes, 1))\n",
        "\n",
        "        self.leaky_relu = nn.LeakyReLU(negative_slope=0.2)\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        if bias:\n",
        "            self.bias = nn.Parameter(torch.Tensor(num_nodes))\n",
        "            nn.init.constant_(self.bias, 0)\n",
        "        else:\n",
        "            self.register_parameter(\"bias\", None)\n",
        "\n",
        "        nn.init.xavier_uniform_(self.a_src, gain=1.414)\n",
        "        nn.init.xavier_uniform_(self.a_dst, gain=1.414)\n",
        "\n",
        "    def forward(self, h):\n",
        "        bs, ch, n, dim = h.size()\n",
        "        h_prime = h\n",
        "        attn_src = torch.matmul(h, self.a_src)\n",
        "        attn_dst = torch.matmul(h, self.a_dst)\n",
        "        attn = attn_src.expand(-1, -1, -1, n) + attn_dst.expand(-1, -1, -1, n).permute(\n",
        "            0, 1, 3, 2\n",
        "        )\n",
        "        attn = self.leaky_relu(attn)\n",
        "        attn = self.softmax(attn)\n",
        "        attn = self.dropout(attn)\n",
        "        output = torch.matmul(attn, h_prime)\n",
        "        return output + self.bias, attn\n",
        "        \n",
        "class T_GMAT(nn.Module):\n",
        "    def __init__(self, n_heads, in_channel, num_nodes, dropout, alpha):\n",
        "        super(T_GMAT, self).__init__()\n",
        "        \n",
        "        self.dropout = dropout\n",
        "        \n",
        "        self.tgmat_layer = T_GMAT_base(\n",
        "                    n_heads, in_channel, num_nodes, dropout\n",
        "                )\n",
        "\n",
        "    def forward(self, x):\n",
        "        bs,ch,n,dim = x.size()\n",
        "        x, attn = self.tgmat_layer(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class T_GMAT_module(nn.Module):\n",
        "    def __init__(self, kern, dilation_factor, temporal_len, n_heads, in_channel, num_nodes, mlp, mlp2, dropout, alpha):\n",
        "        super(T_GMAT_module, self).__init__()\n",
        "        \n",
        "        self.tgmat_net = T_GMAT(n_heads, in_channel, num_nodes, dropout, alpha)\n",
        "\n",
        "        self.mlp_convs = nn.ModuleList()\n",
        "        self.mlp_bns = nn.ModuleList()\n",
        "        last_channel = 32\n",
        "        for out_channel in mlp:\n",
        "            self.mlp_convs.append(nn.Conv2d(last_channel, out_channel, 1))\n",
        "            last_channel = out_channel\n",
        "        \n",
        "        self.mlp_convs2 = nn.ModuleList()\n",
        "        self.mlp_bns2 = nn.ModuleList()\n",
        "        last_channel = n_heads\n",
        "        for out_channel in mlp2:\n",
        "            self.mlp_convs2.append(nn.Conv2d(last_channel, out_channel, 1))\n",
        "            last_channel = out_channel\n",
        "\n",
        "        self.bn_norm2 = nn.BatchNorm2d(out_channel)\n",
        "        self.bn_norm3 = nn.BatchNorm2d(out_channel)\n",
        "\n",
        "        self.dropout1 = nn.Dropout(0.3)\n",
        "        self.dropout2 = nn.Dropout(0.2)\n",
        "\n",
        "        self.mlp = (nn.Conv2d(32,32,(1,kern),dilation=(1,dilation_factor))) \n",
        "  \n",
        "    def forward(self,x):\n",
        "        \n",
        "        bs, ch, n, dim = x.size()\n",
        "        \n",
        "        x_input = x.permute(0,1,3,2)\n",
        "        x_input_cpy = x_input\n",
        "\n",
        "        # Encoder\n",
        "        for i, conv in enumerate(self.mlp_convs):\n",
        "            x_input = F.relu((conv(x_input)))\n",
        "\n",
        "        # T-GMAT\n",
        "        x_input_cpy2 = x_input\n",
        "        x_input = self.tgmat_net(x_input)\n",
        "        x_input = x_input_cpy2+ self.dropout1(x_input)\n",
        "\n",
        "        # Decoder\n",
        "        for i, conv in enumerate(self.mlp_convs2):\n",
        "          x_input = F.relu((conv(x_input)))\n",
        "\n",
        "        x_input = (x_input_cpy + self.dropout2(x_input)).permute(0,1,3,2)\n",
        "\n",
        "        x_input = self.bn_norm2(x_input)\n",
        "\n",
        "        x_input = F.relu(self.mlp(x_input))\n",
        "        x_input = self.bn_norm3(x_input)\n",
        "\n",
        "        return x_input"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wEm25sP_Y41t"
      },
      "source": [
        "### F-GMAT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "code",
        "id": "oHsDnomLY41u"
      },
      "outputs": [],
      "source": [
        "\n",
        "class F_GMAT_base(nn.Module):\n",
        "    def __init__(self, n_heads, in_channel, num_nodes, dropout, bias=True):\n",
        "        super(F_GMAT_base, self).__init__()\n",
        "\n",
        "        self.n_head = n_heads\n",
        "        self.f_in = num_nodes\n",
        "        self.a_src = nn.Parameter(torch.Tensor(self.n_head, num_nodes, 1))\n",
        "        self.a_dst = nn.Parameter(torch.Tensor(self.n_head, num_nodes, 1))\n",
        "\n",
        "        self.leaky_relu = nn.LeakyReLU(negative_slope=0.2)\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        if bias:\n",
        "            self.bias = nn.Parameter(torch.Tensor(num_nodes))\n",
        "            nn.init.constant_(self.bias, 0)\n",
        "        else:\n",
        "            self.register_parameter(\"bias\", None)\n",
        "\n",
        "        nn.init.xavier_uniform_(self.a_src, gain=1.414)\n",
        "        nn.init.xavier_uniform_(self.a_dst, gain=1.414)\n",
        "\n",
        "    def forward(self, h):\n",
        "        bs, ch, n, dim = h.size()\n",
        "        #h_prime = torch.matmul(h, self.w)\n",
        "        h_prime = h\n",
        "        attn_src = torch.matmul(h, self.a_src)\n",
        "        attn_dst = torch.matmul(h, self.a_dst)\n",
        "        attn = attn_src.expand(-1, -1, -1, n) + attn_dst.expand(-1, -1, -1, n).permute(\n",
        "            0, 1, 3, 2\n",
        "        )\n",
        "        attn = self.leaky_relu(attn)\n",
        "        attn = self.softmax(attn)\n",
        "        attn = self.dropout(attn)\n",
        "        output = torch.matmul(attn, h_prime)\n",
        "        return output + self.bias, attn\n",
        "        \n",
        "class F_GMAT(nn.Module):\n",
        "    def __init__(self, n_heads, in_channel, num_nodes, dropout, alpha):\n",
        "        super(F_GMAT, self).__init__()\n",
        "        \n",
        "        self.dropout = dropout\n",
        "        \n",
        "        self.fgmat_layer = F_GMAT_base(\n",
        "                    n_heads, in_channel, num_nodes, dropout\n",
        "                )\n",
        "\n",
        "    def forward(self, x):\n",
        "        bs,ch,n,dim = x.size()\n",
        "        x, attn = self.fgmat_layer(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class F_GMAT_module(nn.Module):\n",
        "    def __init__(self, n_heads, in_channel, num_nodes, mlp, mlp2, dropout, alpha):\n",
        "        super(F_GMAT_module, self).__init__()\n",
        "        self.fgmat_net = F_GMAT(n_heads, in_channel, num_nodes, dropout, alpha)\n",
        "\n",
        "        self.mlp_convs = nn.ModuleList()\n",
        "        self.mlp_bns = nn.ModuleList()\n",
        "        last_channel = 32\n",
        "        for out_channel in mlp:\n",
        "            self.mlp_convs.append(nn.Conv2d(last_channel, out_channel, 1))\n",
        "            last_channel = out_channel\n",
        "        \n",
        "        self.mlp_convs2 = nn.ModuleList()\n",
        "        self.mlp_bns2 = nn.ModuleList()\n",
        "        last_channel = n_heads\n",
        "        for out_channel in mlp2:\n",
        "            self.mlp_convs2.append(nn.Conv2d(last_channel, out_channel, 1))\n",
        "            last_channel = out_channel\n",
        "\n",
        "        self.lay_norm2 = nn.LayerNorm([n_heads,5, num_nodes])\n",
        "\n",
        "        self.dropout1 = nn.Dropout(0.2)\n",
        "        self.dropout2 = nn.Dropout(0.2)\n",
        "    def forward(self,x, x_f1, x_f2, x_f3, x_f4):\n",
        "        bs, ch, n, dim = x.size()\n",
        "      \n",
        "        x_all = []\n",
        "        x_f1_all = []\n",
        "        x_f2_all =[]\n",
        "        x_f3_all = []\n",
        "        x_f4_all = []\n",
        "\n",
        "        # chronological order\n",
        "        for t_idx in range(1,dim):\n",
        "            x_input = [x[:,:,:,t_idx].unsqueeze(2),\n",
        "                    x_f1[:,:,:,t_idx].unsqueeze(2),x_f2[:,:,:,t_idx].unsqueeze(2),\n",
        "                    x_f3[:,:,:,t_idx].unsqueeze(2),x_f4[:,:,:,t_idx].unsqueeze(2)\n",
        "                  ]\n",
        "            x_input = torch.cat(x_input, dim=2)\n",
        "            x_input_cpy = x_input\n",
        "\n",
        "            # Encoder\n",
        "            for i, conv in enumerate(self.mlp_convs):\n",
        "              x_input = F.relu((conv(x_input)))\n",
        "\n",
        "            # F-GAMT\n",
        "            x_input_cpy2 = x_input\n",
        "            x_input = self.fgmat_net(x_input)\n",
        "            x_input = x_input_cpy2+ self.dropout1(x_input)\n",
        "\n",
        "            x_input = self.lay_norm2(x_input)\n",
        "            \n",
        "            # Decoder\n",
        "            for i, conv in enumerate(self.mlp_convs2):\n",
        "              x_input = F.relu((conv(x_input)))\n",
        "\n",
        "            x_input = x_input_cpy+ self.dropout2(x_input)\n",
        "\n",
        "            x_all.append(x_input[:,:,0].unsqueeze(3))\n",
        "            \n",
        "        x_tmp = torch.cat(x_all, dim=3)  # (64,16,207,13)\n",
        "        \n",
        "        x = torch.cat([x[:,:,:,:1],x_tmp],dim=3)\n",
        "        \n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jenRSFIeANli"
      },
      "source": [
        "### Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SEryDF5qANli"
      },
      "outputs": [],
      "source": [
        "\n",
        "def pearson_corr2(all, num_nodes): # all: (64,8,n,dim)\n",
        "  #all = torch.randn(64,8,10,15)\n",
        "  \n",
        "  all_cor = []\n",
        "  for i in range(num_nodes):\n",
        "    a = all[:,i].unsqueeze(1)\n",
        "    b = all.clone()\n",
        "    \n",
        "    corrs = (a*b).mean(axis=-1) - a.mean(axis=-1) * b.mean(axis=-1)\n",
        "    #print(corrs)\n",
        "    std_prod =  torch.sqrt(torch.var(a, dim=-1) * torch.var(b, dim=-1))\n",
        "\n",
        "    out = corrs / std_prod\n",
        "\n",
        "    out = torch.where(torch.isnan(out), 0, out)\n",
        "    \n",
        "    all_cor.append((out).unsqueeze(1))\n",
        "\n",
        "  all_cor = torch.cat(all_cor, dim=1)\n",
        "  all_cor = torch.sigmoid(all_cor)\n",
        "  return all_cor\n",
        "\n",
        "\n",
        "class GMAT_Net(nn.Module):\n",
        "    def __init__(self, \n",
        "                 model_type, \n",
        "                 num_nodes, \n",
        "                 device, \n",
        "                 predefined_A=None,\n",
        "                 dropout=0.3, \n",
        "                 dilation_exponential=1, \n",
        "                 conv_channels=32, \n",
        "                 residual_channels=32, \n",
        "                 skip_channels=64, \n",
        "                 end_channels=128, \n",
        "                 seq_length=12, in_dim=2, out_dim=12, layers=3, layer_norm_affline=True):\n",
        "      \n",
        "        super(GMAT_Net, self).__init__()\n",
        "\n",
        "        self.model_type = model_type\n",
        "\n",
        "        self.num_nodes = num_nodes\n",
        "        self.dropout = dropout\n",
        "        self.predefined_A = predefined_A\n",
        "        self.layers = layers\n",
        "        self.seq_length = seq_length\n",
        "\n",
        "        self.filter_convs = nn.ModuleList()\n",
        "        self.gate_convs = nn.ModuleList()\n",
        "        self.residual_convs = nn.ModuleList()\n",
        "        self.skip_convs = nn.ModuleList()\n",
        "        self.s_gmat = nn.ModuleList()\n",
        "\n",
        "        self.norm = nn.ModuleList()\n",
        "        self.start_conv = nn.Conv2d(in_channels=in_dim,\n",
        "                                    out_channels=residual_channels,\n",
        "                                    kernel_size=(1, 1))\n",
        "        self.f_gmat = nn.ModuleList()\n",
        "        in_channel = 32\n",
        "        n_heads = 8\n",
        "        dropout = 0\n",
        "        alpha = 0.2\n",
        "        self.f_gmat.append(\n",
        "            F_GMAT_module(\n",
        "              n_heads=n_heads, in_channel= in_channel, num_nodes=num_nodes, mlp=[n_heads],mlp2=[32], dropout=dropout, alpha=alpha\n",
        "            )\n",
        "        )\n",
        "\n",
        "        self.t_gmat1 = nn.ModuleList()\n",
        "        self.t_gmat2 = nn.ModuleList()\n",
        "\n",
        "        self.receptive_field = 13\n",
        "        print(\"# Model Type\", self.model_type)\n",
        "        print(\"# receptive_field\", self.receptive_field)\n",
        "        i=0\n",
        "\n",
        "        target_len = 13\n",
        "        for j in range(1,layers+1):\n",
        "           \n",
        "            kern = 5\n",
        "\n",
        "            dilation_factor = 1\n",
        "            \n",
        "            in_channel = 32\n",
        "            n_heads = 8\n",
        "            dropout = 0\n",
        "            alpha = 0.2\n",
        "            self.t_gmat1.append(\n",
        "                T_GMAT_module(\n",
        "                  kern= kern, dilation_factor=dilation_factor, temporal_len = target_len, n_heads=n_heads, in_channel= in_channel, num_nodes=num_nodes, mlp=[n_heads],mlp2=[32], dropout=dropout, alpha=alpha\n",
        "                )\n",
        "            )\n",
        "            \n",
        "            self.t_gmat2.append(\n",
        "                T_GMAT_module(\n",
        "                  kern= kern, dilation_factor=dilation_factor, temporal_len = target_len, n_heads=n_heads, in_channel= in_channel, num_nodes=num_nodes, mlp=[n_heads],mlp2=[32], dropout=dropout, alpha=alpha\n",
        "                )\n",
        "            )\n",
        "            target_len -= 4\n",
        "\n",
        "            self.skip_convs.append(nn.Conv2d(in_channels=conv_channels,\n",
        "                                            out_channels=skip_channels,\n",
        "                                            kernel_size=(1, target_len)))\n",
        "\n",
        "            in_channel = 32\n",
        "            n_heads = 8\n",
        "            dropout = 0\n",
        "            alpha = 0.2\n",
        "            \n",
        "            depth = 2\n",
        "            self.s_gmat.append(\n",
        "                S_GMAT_module(\n",
        "                  depth=depth, temporal_len = target_len, n_heads=n_heads, in_channel= in_channel, num_nodes=num_nodes, mlp=[n_heads],mlp2=[32], dropout=dropout, alpha=alpha\n",
        "                )\n",
        "            )\n",
        "\n",
        "            self.norm.append(LayerNorm((residual_channels, num_nodes, target_len),elementwise_affine=layer_norm_affline))\n",
        "        \n",
        "        self.end_conv_1 = nn.Conv2d(in_channels=skip_channels,\n",
        "                                             out_channels=end_channels,\n",
        "                                             kernel_size=(1,1),\n",
        "                                             bias=True)\n",
        "        self.end_conv_2 = nn.Conv2d(in_channels=end_channels,\n",
        "                                             out_channels=out_dim,\n",
        "                                             kernel_size=(1,1),\n",
        "                                             bias=True)\n",
        "        \n",
        "        if self.seq_length > self.receptive_field:\n",
        "            self.skip0 = nn.Conv2d(in_channels=in_dim, out_channels=skip_channels, kernel_size=(1, self.seq_length), bias=True)\n",
        "            self.skipE = nn.Conv2d(in_channels=residual_channels, out_channels=skip_channels, kernel_size=(1, self.seq_length-self.receptive_field+1), bias=True)\n",
        "\n",
        "        else:\n",
        "            self.skip0 = nn.Conv2d(in_channels=in_dim, out_channels=skip_channels, kernel_size=(1, self.receptive_field), bias=True)\n",
        "            self.skipE = nn.Conv2d(in_channels=residual_channels, out_channels=skip_channels, kernel_size=(1, 1), bias=True)\n",
        "\n",
        "        self.idx = torch.arange(self.num_nodes).to(device)\n",
        "\n",
        "    def forward(self, input, input_f1,input_f2,input_f3,input_f4, idx=None):\n",
        "        seq_len = input.size(3)\n",
        "        assert seq_len==self.seq_length, 'input sequence length not equal to preset sequence length'\n",
        "\n",
        "        bs,c,n,dim= input.shape\n",
        "\n",
        "        # Pearson Correlation\n",
        "        pearson_attn = pearson_corr2(input[:,0], n).unsqueeze(1) #input: 64,n,dim, attn: 64,n,n\n",
        "\n",
        "        # Step0: 檢查receptive_field, 不足則padding0\n",
        "        if self.seq_length<self.receptive_field:\n",
        "            input = nn.functional.pad(input,(self.receptive_field-self.seq_length,0,0,0))\n",
        "            input_f1 = nn.functional.pad(input_f1,(self.receptive_field-self.seq_length,0,0,0))\n",
        "            input_f2 = nn.functional.pad(input_f2,(self.receptive_field-self.seq_length,0,0,0))\n",
        "            input_f3 = nn.functional.pad(input_f3,(self.receptive_field-self.seq_length,0,0,0))\n",
        "            input_f4 = nn.functional.pad(input_f4,(self.receptive_field-self.seq_length,0,0,0))\n",
        "\n",
        "        # Step1: turn([64, 2, 207, 13]) to ([64, 32, 207, 13]) => 固定用同一conv\n",
        "        x = self.start_conv(input) \n",
        "        x_f1 = self.start_conv(input_f1)  \n",
        "        x_f2 = self.start_conv(input_f2)\n",
        "        x_f3 = self.start_conv(input_f3)  \n",
        "        x_f4 = self.start_conv(input_f4) \n",
        "\n",
        "        x = self.f_gmat[0](x,x_f1,x_f2,x_f3,x_f4)\n",
        "\n",
        "        skip = self.skip0(F.dropout(input, self.dropout, training=self.training))\n",
        "        \n",
        "        # Layers : 3層 : 19->13->7->1 (取決於TCN取的維度)\n",
        "        for i in range(self.layers):\n",
        "            \n",
        "            residual = x    \n",
        "            \n",
        "            ### T-GMAT -START###\n",
        "            filter = self.t_gmat1[i](x)\n",
        "            filter = torch.tanh(filter)\n",
        "\n",
        "            gate = self.t_gmat2[i](x)\n",
        "            gate = torch.sigmoid(gate)\n",
        "            ### T-GMAT -END###\n",
        "\n",
        "            x = filter * gate\n",
        "            x = F.dropout(x, self.dropout, training=self.training)\n",
        "\n",
        "            s = x\n",
        "            s = self.skip_convs[i](s)    \n",
        "            skip = s + skip\n",
        "            \n",
        "            ### S-GMAT -START###\n",
        "            x = self.s_gmat[i](x, self.predefined_A[0], self.predefined_A[1], pearson_attn)\n",
        "            ### S-GMAT -START###\n",
        "\n",
        "            x = x + residual[:, :, :, -x.size(3):]\n",
        "            \n",
        "            # Based on MTGNN: https://github.com/nnzhan/MTGNN\n",
        "            if idx is None:\n",
        "                x = self.norm[i](x,self.idx)\n",
        "            else:\n",
        "                x = self.norm[i](x,idx)\n",
        "\n",
        "        skip = self.skipE(x) + skip\n",
        "\n",
        "        # Based on MTGNN\n",
        "        ### Output Module -START### \n",
        "        x = F.relu(skip)\n",
        "        x = F.relu(self.end_conv_1(x))\n",
        "        x = self.end_conv_2(x)\n",
        "        ### Output Module -START###\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbowdbREANli"
      },
      "source": [
        "### Trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BTga9hrXANli"
      },
      "outputs": [],
      "source": [
        "class Trainer():\n",
        "    def __init__(self, model, lrate, wdecay, clip, step_size, seq_out_len, scaler, device, cl=True):\n",
        "        self.scaler = scaler\n",
        "        self.model = model\n",
        "        self.model.to(device)\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), lr=lrate, weight_decay=wdecay)\n",
        "        self.loss = masked_mae\n",
        "        self.clip = clip\n",
        "        self.step = step_size\n",
        "        self.iter = 1\n",
        "        self.task_level = 1\n",
        "        self.seq_out_len = seq_out_len\n",
        "        self.cl = cl\n",
        "\n",
        "    def train(self, input, input_f1, input_f2, input_f3, input_f4 ,real_val, idx=None):\n",
        "        self.model.train()\n",
        "        self.optimizer.zero_grad()\n",
        "        output = self.model(input, input_f1, input_f2, input_f3,input_f4, idx=idx)\n",
        "        output = output.transpose(1,3)\n",
        "        real = torch.unsqueeze(real_val,dim=1)\n",
        "        predict = self.scaler.inverse_transform(output)\n",
        "        \n",
        "        #for i in range(args.num_nodes):\n",
        "        #  predict[:,0,i,:] = self.scaler[i].inverse_transform(predict[:,0,i,:])\n",
        "\n",
        "        if self.iter%self.step==0 and self.task_level<=self.seq_out_len:\n",
        "            self.task_level +=1\n",
        "            print(\"### cl learning\\n iter\",self.iter,\"\\niter%step\",self.iter%self.step,\"\\ntask_level\",self.task_level)\n",
        "            print(\"# predict len:\", len(predict[:, :, :, :self.task_level]))\n",
        "        \n",
        "        if self.cl:\n",
        "            loss = masked_mae(predict[:, :, :, :self.task_level], real[:, :, :, :self.task_level], 0.0)\n",
        "        else:\n",
        "            loss = masked_mae(predict, real, 0.0)\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        if self.clip is not None:\n",
        "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.clip)\n",
        "\n",
        "        self.optimizer.step()\n",
        "        \n",
        "        '''\n",
        "        mae = masked_mae(predict,real,0.0).item()\n",
        "        mape = masked_mape(predict,real,0.0).item()\n",
        "        rmse = masked_rmse(predict,real,0.0).item()\n",
        "        smape = masked_smape(predict,real,0.0).item()\n",
        "        '''\n",
        "        metrics = metric(predict, real) # mae,mape,rmse,smape\n",
        "        \n",
        "        self.iter += 1\n",
        "        return metrics # mae,mape,rmse,smape\n",
        "\n",
        "    def eval(self, input, input_f1, input_f2, input_f3,input_f4, real_val):\n",
        "        self.model.eval()\n",
        "        output = self.model(input, input_f1, input_f2, input_f3,input_f4)\n",
        "        output = output.transpose(1,3)\n",
        "        real = torch.unsqueeze(real_val,dim=1)\n",
        "        predict = self.scaler.inverse_transform(output)\n",
        "        \n",
        "        '''\n",
        "        loss = self.loss(predict, real, 0.0)\n",
        "        mape = masked_mape(predict,real,0.0).item()\n",
        "        rmse = masked_rmse(predict,real,0.0).item()\n",
        "        '''    \n",
        "        metrics = metric(predict, real) # mae,mape,rmse,smape\n",
        "        return metrics # mae,mape,rmse,smape\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_84y8rqsANlj"
      },
      "source": [
        "### Parameter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UQX_bh_FANlj"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def str_to_bool(value):\n",
        "    if isinstance(value, bool):\n",
        "        return value\n",
        "    if value.lower() in {'false', 'f', '0', 'no', 'n'}:\n",
        "        return False\n",
        "    elif value.lower() in {'true', 't', '1', 'yes', 'y'}:\n",
        "        return True\n",
        "    raise ValueError(f'{value} is not a valid boolean value')\n",
        "\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('--device',type=str,default='cuda',help='')\n",
        "parser.add_argument('--adjtype',type=str,default='doubletransition',help='adj type')\n",
        "\n",
        "parser.add_argument('--cl', type=str_to_bool, default=True,help='whether to do curriculum learning')\n",
        "\n",
        "parser.add_argument('--dropout',type=float,default=0.3,help='dropout rate')\n",
        "\n",
        "parser.add_argument('--conv_channels',type=int,default=32,help='convolution channels')\n",
        "parser.add_argument('--residual_channels',type=int,default=32,help='residual channels')\n",
        "\n",
        "parser.add_argument('--in_dim',type=int,default=2,help='inputs dimension')\n",
        "parser.add_argument('--seq_in_len',type=int,default=12,help='input sequence length')\n",
        "parser.add_argument('--seq_out_len',type=int,default=12,help='output sequence length')\n",
        "\n",
        "\n",
        "parser.add_argument('--batch_size',type=int,default=64,help='batch size')\n",
        "parser.add_argument('--clip',type=int,default=5,help='clip')\n",
        "\n",
        "\n",
        "parser.add_argument('--model_type',type=str,default='GMAT_Net',help='model type')\n",
        "parser.add_argument('--skip_channels',type=int,default=64,help='skip channels')\n",
        "parser.add_argument('--end_channels',type=int,default=128,help='end channels')\n",
        "parser.add_argument('--layers',type=int,default=3,help='number of layers')\n",
        "\n",
        "\n",
        "parser.add_argument('--print_every',type=int,default=50,help='')\n",
        "parser.add_argument('--seed',type=int,default=101,help='random seed')\n",
        "parser.add_argument('--save',type=str,default='./save/',help='save path')\n",
        "\n",
        "parser.add_argument('--log_print', type=str_to_bool, default=False ,help='whether to load static feature')\n",
        "\n",
        "parser.add_argument('--learning_rate',type=float,default=0.0005,help='learning rate')\n",
        "parser.add_argument('--weight_decay',type=float,default=0.0001,help='weight decay rate')\n",
        "\n",
        "target = 'RMThsin'\n",
        "parser.add_argument('--data',type=str,default='Data' ,help='data path')\n",
        "parser.add_argument('--adj_data',type=str,default='Data/adj_mat_'+target+'.pkl',help='adj data path')\n",
        "parser.add_argument('--num_nodes',type=int,default=11,help='number of nodes/variables')\n",
        "\n",
        "parser.add_argument('--step_size1',type=int,default=1500,help='step_size')\n",
        "parser.add_argument('--step_size2',type=int,default=100,help='step_size')\n",
        "\n",
        "parser.add_argument('--expid',type=int,default=202302051145,help='experiment id')\n",
        "parser.add_argument('--runs',type=int,default=2,help='number of runs')\n",
        "parser.add_argument('--epochs',type=int,default=150,help='')\n",
        "\n",
        "args=parser.parse_args(args=[])\n",
        "torch.set_num_threads(3)\n",
        "\n",
        "args=parser.parse_args(args=[])\n",
        "print('# args', args)\n",
        "\n",
        "device = torch.device(args.device)\n",
        "\n",
        "writer = SummaryWriter()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82Pq4DDjV9Bt"
      },
      "source": [
        "### Loading Data: Univariate+MA+GA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jsioPbguV9Bu"
      },
      "outputs": [],
      "source": [
        "\"\"\"### Loading Data\"\"\"\n",
        "\n",
        "batch_size = args.batch_size\n",
        "valid_batch_size = args.batch_size\n",
        "test_batch_size = args.batch_size\n",
        "data = {}\n",
        "\n",
        "augmented_features = ['','_ma3','_ma6','_ga12','_ga24']\n",
        "key_name = ['','_f1','_f2','_f3','_f4']\n",
        "\n",
        "for i in range(len(augmented_features)):\n",
        "    print(\"range_type\", augmented_features[i])\n",
        "    for category in ['train', 'val', 'test']:\n",
        "        key = category + key_name[i]\n",
        "        key2 = category + augmented_features[i]\n",
        "        # Loading npz \n",
        "        cat_data = np.load(os.path.join(args.data, key2 + '.npz'))\n",
        "        print(\"loading... key:\", key ,'->', args.data, key2 + '.npz')\n",
        "\n",
        "        if category == \"train\":\n",
        "          data['x_' + key] = cat_data['x'][:]     # (?, 12, 207, 2)\n",
        "          data['y_' + key] = cat_data['y'][:]   # (?, 12, 207, 2)\n",
        "        else:\n",
        "          data['x_' + key] = cat_data['x']     # (?, 12, 207, 2)\n",
        "          data['y_' + key] = cat_data['y']     # (?, 12, 207, 2)\n",
        "    \n",
        "    print(data.keys())\n",
        "    if augmented_features[i] == '':\n",
        "        # 使用train的mean/std來正規化valid/test #\n",
        "        scaler = StandardScaler(mean=data['x_train'][..., 0].mean(), std=data['x_train'][..., 0].std())\n",
        "        data['scaler'] = scaler\n",
        "        \n",
        "    # 將欲訓練特徵改成正規化\n",
        "    for category in ['train', 'val', 'test']:\n",
        "        key = category + key_name[i]\n",
        "        data['x_' + key][..., 0] = data['scaler'].transform(data['x_' + key][..., 0])\n",
        "        print(\"data['x_' + key]:\", 'x_' + key)\n",
        "\n",
        "data['train_loader'] = DataLoaderM(\n",
        "    data['x_train'], data['y_train'], \n",
        "    data['x_train_f1'], \n",
        "    data['x_train_f2'], \n",
        "    data['x_train_f3'],\n",
        "    data['x_train_f4'],\n",
        "    batch_size)\n",
        "\n",
        "data['val_loader'] = DataLoaderM(\n",
        "    data['x_val'], data['y_val'], \n",
        "    data['x_val_f1'],  \n",
        "    data['x_val_f2'],  \n",
        "    data['x_val_f3'],  \n",
        "    data['x_val_f4'],  \n",
        "    valid_batch_size)\n",
        "\n",
        "data['test_loader'] = DataLoaderM(\n",
        "    data['x_test'], data['y_test'], \n",
        "    data['x_test_f1'],  \n",
        "    data['x_test_f2'], \n",
        "    data['x_test_f3'],\n",
        "    data['x_test_f4'],  \n",
        "    test_batch_size)\n",
        "\n",
        "print(data.keys())\n",
        "'''\n",
        "adj_mx: 根據distances_la_2012.csv, 找出每個sensor與其他sensor距離並建立距離矩陣, 再進行正規化\n",
        "'''\n",
        "sensor_ids, sensor_id_to_ind, adj_mx = load_adj(args.adj_data,args.adjtype)   # adjtype: default='doubletransition'\n",
        "\n",
        "adj_mx = [torch.tensor(i).to(device) for i in adj_mx]\n",
        "\n",
        "dataloader = data.copy()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4CsL3F0KxBn"
      },
      "source": [
        "### Training Model (GMAT-Net)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_UkPMVHVKxBt"
      },
      "outputs": [],
      "source": [
        "\n",
        "def main(runid):\n",
        "    \n",
        "    model = GMAT_Net(\n",
        "        args.model_type, \n",
        "        args.num_nodes,\n",
        "        device, \n",
        "        predefined_A=adj_mx, \n",
        "        dropout=args.dropout, \n",
        "        conv_channels=args.conv_channels, \n",
        "        residual_channels=args.residual_channels,\n",
        "        skip_channels=args.skip_channels, \n",
        "        end_channels= args.end_channels,\n",
        "        seq_length=args.seq_in_len, in_dim=args.in_dim, out_dim=args.seq_out_len,  layers=args.layers, layer_norm_affline=True)\n",
        "\n",
        "\n",
        "    nParams = sum([p.nelement() for p in model.parameters()])       # model參數量!\n",
        "    print('Number of model parameters is', nParams)\n",
        "\n",
        "    engine = Trainer(model, args.learning_rate, args.weight_decay, args.clip, args.step_size1, args.seq_out_len, data['scaler'], device, args.cl)\n",
        "    \n",
        "    print(\"start training...\",flush=True)\n",
        "    his_loss =[]\n",
        "    val_time = []\n",
        "    train_time = []\n",
        "    minl = 1e5\n",
        "    start_epoch=0\n",
        "    train_loss_epoch = []  # 紀錄train在epoch收斂\n",
        "    valid_loss_epoch = []  # 紀錄valid在epoch收斂\n",
        "    \n",
        "    '''\n",
        "    #####\n",
        "    SAVE_PATH = args.save + \"exp202112290828_0.pth\" \n",
        "    checkpoint = torch.load(SAVE_PATH)\n",
        "    engine.model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    engine.task_level = checkpoint['task_level']\n",
        "    start_epoch = checkpoint['epoch']\n",
        "    train_loss_epoch = checkpoint['train_loss']\n",
        "    valid_loss_epoch = checkpoint['valid_loss']\n",
        "    #####\n",
        "    '''\n",
        "    \n",
        "    for i in range(start_epoch,start_epoch+args.epochs+1):\n",
        "        train_mae = []\n",
        "        train_mape = []\n",
        "        train_smape = []\n",
        "        train_rmse = []\n",
        "        t1 = time.time()\n",
        "        dataloader['train_loader'].shuffle()  # 為了檢視資料先拿掉\n",
        "        for iter, (x, y,x_f1,x_f2,x_f3,x_f4) in enumerate(dataloader['train_loader'].get_iterator()):\n",
        "            trainx = torch.Tensor(x).to(device)\n",
        "            trainx= trainx.transpose(1, 3)\n",
        "            trainy = torch.Tensor(y).to(device)\n",
        "            trainy = trainy.transpose(1, 3)\n",
        "            \n",
        "            trainx_f1 = torch.Tensor(x_f1).to(device)\n",
        "            trainx_f1= trainx_f1.transpose(1, 3)\n",
        "            \n",
        "            trainx_f2 = torch.Tensor(x_f2).to(device)\n",
        "            trainx_f2= trainx_f2.transpose(1, 3)\n",
        "            \n",
        "            \n",
        "            trainx_f3 = torch.Tensor(x_f3).to(device)\n",
        "            trainx_f3= trainx_f3.transpose(1, 3)\n",
        "            \n",
        "            trainx_f4 = torch.Tensor(x_f4).to(device)\n",
        "            trainx_f4= trainx_f4.transpose(1, 3)\n",
        "            \n",
        "            #mae,mape,rmse,smape\n",
        "            metrics = engine.train(trainx,trainx_f1,trainx_f2,trainx_f3,trainx_f4 ,trainy[:,0,:,:])\n",
        "\n",
        "            train_mae.append(metrics[0])\n",
        "            train_mape.append(metrics[1])\n",
        "            train_rmse.append(metrics[2])\n",
        "            train_smape.append(metrics[3])\n",
        "\n",
        "            if iter % args.print_every == 0 :\n",
        "                log = 'Iter: {:03d}, Train MAE: {:.4f}, Train MAPE: {:.4f}, Train RMSE: {:.4f}, *Train SMAPE: {:.4f}'\n",
        "                print(log.format(iter, train_mae[-1], train_mape[-1], train_rmse[-1], train_smape[-1]),flush=True)\n",
        "        t2 = time.time()\n",
        "        train_time.append(t2-t1)\n",
        "        #validation\n",
        "        valid_mae = []\n",
        "        valid_mape = []\n",
        "        valid_rmse = []\n",
        "        valid_smape = []\n",
        "\n",
        "        s1 = time.time()\n",
        "        for iter, (x, y,x_f1,x_f2,x_f3,x_f4)  in enumerate(dataloader['val_loader'].get_iterator()):\n",
        "            testx = torch.Tensor(x).to(device)\n",
        "            testx = testx.transpose(1, 3)\n",
        "            testy = torch.Tensor(y).to(device)\n",
        "            testy = testy.transpose(1, 3)\n",
        "            \n",
        "            testx_f1 = torch.Tensor(x_f1).to(device)\n",
        "            testx_f1= testx_f1.transpose(1, 3)\n",
        "            \n",
        "            testx_f2 = torch.Tensor(x_f2).to(device)\n",
        "            testx_f2= testx_f2.transpose(1, 3)\n",
        "            \n",
        "            \n",
        "            testx_f3 = torch.Tensor(x_f3).to(device)\n",
        "            testx_f3= testx_f3.transpose(1, 3)\n",
        "            \n",
        "            testx_f4 = torch.Tensor(x_f4).to(device)\n",
        "            testx_f4= testx_f4.transpose(1, 3)\n",
        "            \n",
        "            \n",
        "            metrics = engine.eval(testx, testx_f1,testx_f2,testx_f3,testx_f4, testy[:,0,:,:])\n",
        "            valid_mae.append(metrics[0])\n",
        "            valid_mape.append(metrics[1])\n",
        "            valid_rmse.append(metrics[2])\n",
        "            valid_smape.append(metrics[3])\n",
        "            \n",
        "        s2 = time.time()\n",
        "        log = 'Epoch: {:03d}, Inference Time: {:.4f} secs'\n",
        "        print(log.format(i,(s2-s1)))\n",
        "        val_time.append(s2-s1)\n",
        "        mtrain_mae = np.mean(train_mae)\n",
        "        mtrain_mape = np.mean(train_mape)\n",
        "        mtrain_rmse = np.mean(train_rmse)\n",
        "        mtrain_smape = np.mean(train_smape)\n",
        "\n",
        "        mvalid_mae = np.mean(valid_mae)\n",
        "        mvalid_mape = np.mean(valid_mape)\n",
        "        mvalid_rmse = np.mean(valid_rmse)\n",
        "        mvalid_smape = np.mean(valid_smape)\n",
        "        \n",
        "        #his_loss.append(mvalid_loss)\n",
        "        his_loss.append(mvalid_smape)\n",
        "\n",
        "        log = 'Epoch: {:03d}, Train MAE: {:.4f}, Train MAPE: {:.4f}, Train RMSE: {:.4f}, *Train SMAPE: {:.4f}, Valid MAE: {:.4f}, Valid MAPE: {:.4f}, Valid RMSE: {:.4f}, *Valid SMAPE: {:.4f}, Training Time: {:.4f}/epoch'\n",
        "        print(log.format(i, mtrain_mae, mtrain_mape, mtrain_rmse, mtrain_smape, mvalid_mae, mvalid_mape, mvalid_rmse, mvalid_smape, (t2 - t1)),flush=True)\n",
        "        \n",
        "        train_loss_epoch.append(mtrain_mae)\n",
        "        valid_loss_epoch.append(mvalid_mae)\n",
        "        \n",
        "        if mvalid_mae<minl:\n",
        "            target_best_model = args.save + \"exp\" + str(args.expid) + \"_\" + str(runid) +\".pth\"\n",
        "            print(\"### Update Best Model:\",target_best_model, '*LOSS:', mvalid_smape, \" ###\")\n",
        "            SAVE_PATH = args.save + \"exp\" + str(args.expid) + \"_\" + str(runid) +\".pth\"\n",
        "            torch.save({\n",
        "              'epoch': i,\n",
        "              'task_level': engine.task_level,\n",
        "              'model_state_dict': engine.model.state_dict(),\n",
        "              'optimizer_state_dict': engine.optimizer.state_dict(),\n",
        "              'loss': mvalid_mae,\n",
        "              'train_loss': train_loss_epoch,\n",
        "              'valid_loss': valid_loss_epoch\n",
        "            }, SAVE_PATH)\n",
        "            minl = mvalid_mae\n",
        "\n",
        "    print(\"Average Training Time: {:.4f} secs/epoch\".format(np.mean(train_time)))\n",
        "    print(\"Average Inference Time: {:.4f} secs\".format(np.mean(val_time)))\n",
        "\n",
        "\n",
        "    bestid = np.argmin(his_loss)\n",
        "    \n",
        "\n",
        "    print(\"Training finished\")\n",
        "    print(\"The valid loss on best model is\", str(round(his_loss[bestid],4)))\n",
        "    \n",
        "    target_model = args.save + \"exp\" + str(args.expid) + \"_\" + str(runid) +\".pth\"\n",
        "   \n",
        "    print(\"### loading model is:\",target_model ,'###')\n",
        "    \n",
        "    \n",
        "    SAVE_PATH = args.save + \"exp\" + str(args.expid) + \"_\" + str(runid) +\".pth\"\n",
        "    \n",
        "    checkpoint = torch.load(SAVE_PATH)\n",
        "    engine.model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    engine.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    engine.task_level = checkpoint['task_level']\n",
        "    start_epoch = checkpoint['epoch']\n",
        "    train_loss_epoch = checkpoint['train_loss']\n",
        "    valid_loss_epoch = checkpoint['valid_loss']\n",
        "    \n",
        "\n",
        "    ### 測試讀取出的model ### \n",
        "    valid_mae = []  \n",
        "    valid_mape = [] \n",
        "    valid_smape = []    \n",
        "    valid_rmse = [] \n",
        "    tmp_y = []\n",
        "    for iter, (x, y,x_f1,x_f2,x_f3,x_f4)  in enumerate(dataloader['val_loader'].get_iterator()):  \n",
        "        \n",
        "        testx = torch.Tensor(x).to(device)  \n",
        "        testx = testx.transpose(1, 3)   \n",
        "        testy = torch.Tensor(y).to(device)  \n",
        "        testy = testy.transpose(1, 3)   \n",
        "        \n",
        "        testx_f1 = torch.Tensor(x_f1).to(device)\n",
        "        testx_f1= testx_f1.transpose(1, 3)\n",
        "\n",
        "        testx_f2 = torch.Tensor(x_f2).to(device)\n",
        "        testx_f2= testx_f2.transpose(1, 3)\n",
        "\n",
        "        testx_f3 = torch.Tensor(x_f3).to(device)\n",
        "        testx_f3= testx_f3.transpose(1, 3)\n",
        "        \n",
        "        testx_f4 = torch.Tensor(x_f4).to(device)\n",
        "        testx_f4= testx_f4.transpose(1, 3)\n",
        "\n",
        "        metrics = engine.eval(testx, testx_f1,testx_f2, testx_f3,testx_f4,testy[:,0,:,:]) \n",
        "        valid_mae.append(metrics[0])    \n",
        "        valid_mape.append(metrics[1])   \n",
        "        valid_rmse.append(metrics[2])   \n",
        "        valid_smape.append(metrics[3])\n",
        "\n",
        "    mvalid_mae = np.mean(valid_mae) \n",
        "    mvalid_mape = np.mean(valid_mape)   \n",
        "    mvalid_rmse = np.mean(valid_rmse)   \n",
        "    mvalid_smape = np.mean(valid_smape) \n",
        "    print(\"### 2-The valid loss on loding model is\", str(round(mvalid_smape,4)))\n",
        "    minl= valid_smape   \n",
        "    print(\"### minl:\",minl, \"checkpoint['loss']:\",checkpoint['loss'])   \n",
        "    ### 測試讀取出的model ### \n",
        "\n",
        "    #valid data\n",
        "    outputs = []\n",
        "    realy = torch.Tensor(dataloader['y_val']).to(device)\n",
        "    \n",
        "    realy = realy.transpose(1,3)[:,0,:,:]\n",
        "    print('#realy', realy.shape)\n",
        "    \n",
        "    for iter, (x, y,x_f1,x_f2,x_f3,x_f4)  in enumerate(dataloader['val_loader'].get_iterator()):\n",
        "        testx = torch.Tensor(x).to(device)\n",
        "        testx = testx.transpose(1,3)\n",
        "        \n",
        "        testx_f1 = torch.Tensor(x_f1).to(device)\n",
        "        testx_f1= testx_f1.transpose(1, 3)\n",
        "\n",
        "        testx_f2 = torch.Tensor(x_f2).to(device)\n",
        "        testx_f2= testx_f2.transpose(1, 3)\n",
        "\n",
        "        testx_f3 = torch.Tensor(x_f3).to(device)\n",
        "        testx_f3= testx_f3.transpose(1, 3)\n",
        "\n",
        "        testx_f4 = torch.Tensor(x_f4).to(device)\n",
        "        testx_f4= testx_f4.transpose(1, 3)\n",
        "        with torch.no_grad():\n",
        "            preds = engine.model(testx,testx_f1,testx_f2,testx_f3,testx_f4)\n",
        "            preds = preds.transpose(1,3)  # 64,1,6,12\n",
        "\n",
        "        outputs.append(preds.squeeze()) # 64,1,6,12 ->squeeze()->64,6,12\n",
        "\n",
        "    yhat = torch.cat(outputs,dim=0)\n",
        "    yhat = yhat[:realy.size(0),...]  # 5240,6,12\n",
        "    print('# cat valid preds', yhat.shape)\n",
        "\n",
        "    pred = dataloader['scaler'].inverse_transform(yhat)\n",
        "    \n",
        "    vmae, vmape, vrmse,vsmape = metric(pred,realy)\n",
        "    print(\"valid - vmae, vmape, vrmse,vsmape\", vmae, vmape, vrmse,vsmape)\n",
        "    #----------------------------------#\n",
        "    #test data\n",
        "    outputs = []\n",
        "    realy = torch.Tensor(dataloader['y_test']).to(device)\n",
        "    realy = realy.transpose(1, 3)[:, 0, :, :]\n",
        "\n",
        "    for iter, (x, y,x_f1,x_f2,x_f3,x_f4)  in enumerate(dataloader['test_loader'].get_iterator()):\n",
        "        testx = torch.Tensor(x).to(device)\n",
        "        testx = testx.transpose(1, 3)\n",
        "        \n",
        "        testx_f1 = torch.Tensor(x_f1).to(device)\n",
        "        testx_f1= testx_f1.transpose(1, 3)\n",
        "\n",
        "        testx_f2 = torch.Tensor(x_f2).to(device)\n",
        "        testx_f2= testx_f2.transpose(1, 3)\n",
        "\n",
        "        testx_f3 = torch.Tensor(x_f3).to(device)\n",
        "        testx_f3= testx_f3.transpose(1, 3)\n",
        "\n",
        "        testx_f4 = torch.Tensor(x_f4).to(device)\n",
        "        testx_f4= testx_f4.transpose(1, 3)\n",
        "\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            preds = engine.model(testx,testx_f1,testx_f2,testx_f3,testx_f4)\n",
        "            preds = preds.transpose(1, 3)\n",
        "        outputs.append(preds.squeeze())\n",
        "\n",
        "    yhat = torch.cat(outputs, dim=0)\n",
        "    yhat = yhat[:realy.size(0), ...]  #10478, 6, 12\n",
        "    print('# cat test preds', yhat.shape)\n",
        "    \n",
        "    mae = []\n",
        "    mape = []\n",
        "    rmse = []\n",
        "    smape = []\n",
        "    \n",
        "    for i in range(args.seq_out_len):\n",
        "        pred = dataloader['scaler'].inverse_transform(yhat[:, :, i])\n",
        "        \n",
        "        real = realy[:, :, i]\n",
        "\n",
        "        metrics = metric(pred, real)\n",
        "        \n",
        "        log = 'Evaluate best model on test data for horizon {:d}, Test MAE: {:.4f}, Test MAPE: {:.4f}, Test RMSE: {:.4f}, Test SMAPE: {:.4f}'\n",
        "        print(log.format(i + 1, metrics[0], metrics[1], metrics[2], metrics[3]))\n",
        "        mae.append(metrics[0])\n",
        "        mape.append(metrics[1])\n",
        "        rmse.append(metrics[2])\n",
        "        smape.append(metrics[3])\n",
        "        \n",
        "    #sys.exit()\n",
        "    log = '{:.2f}   {:.2f}  {:.4f}  {:.4f}  '\n",
        "    print(\"#### Final Results:\")\n",
        "    print(  str(args.expid) + \"_\" + str(runid)+'    ', \n",
        "          log.format(mae[0], rmse[0], smape[0], mape[0]),\n",
        "          log.format(mae[2], rmse[2], smape[2], mape[2]),\n",
        "          log.format(mae[5], rmse[5], smape[5], mape[5]),\n",
        "          log.format(mae[11], rmse[11], smape[11], mape[11]),\n",
        "         )\n",
        "    ### Drawing Loss Diagram ###\n",
        "    fig = plt.figure(figsize=(10, 6), dpi=600)\n",
        "    plt.plot(checkpoint['train_loss'], label=\"train loss\")\n",
        "    plt.plot(checkpoint['valid_loss'], label=\"valid loss\")\n",
        "    plt.legend(loc=\"upper right\")\n",
        "    plt.title('#Loss of Training', fontsize=20)\n",
        "    plt.ylabel(\"MAE\", fontsize=14)\n",
        "    plt.xlabel(\"Epochs\", fontsize=14)\n",
        "    plt.show()\n",
        "    return vmae, vmape, vrmse,vsmape, mae, mape, rmse, smape\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    vmae = []\n",
        "    vmape = []\n",
        "    vrmse = []\n",
        "    vsmape = []\n",
        "    mae = []\n",
        "    mape = []\n",
        "    rmse = []\n",
        "    smape = []\n",
        "    for i in range(args.runs):\n",
        "        vm1, vm2, vm3, vm4, m1, m2, m3, m4 = main(i)\n",
        "        vmae.append(vm1)\n",
        "        vmape.append(vm2)\n",
        "        vrmse.append(vm3)\n",
        "        vsmape.append(vm4)\n",
        "        mae.append(m1)\n",
        "        mape.append(m2)\n",
        "        rmse.append(m3)\n",
        "        smape.append(m4)\n",
        "\n",
        "    mae = np.array(mae)\n",
        "    mape = np.array(mape)\n",
        "    rmse = np.array(rmse)\n",
        "    smape = np.array(smape)\n",
        "\n",
        "    amae = np.mean(mae,0)\n",
        "    amape = np.mean(mape,0)\n",
        "    armse = np.mean(rmse,0)\n",
        "    asmape = np.mean(smape,0)\n",
        "\n",
        "    smae = np.std(mae,0)\n",
        "    s_mape = np.std(mape,0)\n",
        "    srmse = np.std(rmse,0)\n",
        "    ssmape = np.std(smape,0)\n",
        "\n",
        "    print('\\n\\nResults for runs\\n\\n')\n",
        "    #valid data\n",
        "    print('valid\\tMAE\\tRMSE\\tMAPE')\n",
        "    log = 'mean:\\t{:.4f}\\t{:.4f}\\t{:.4f}\\t{:.4f}'\n",
        "    print(log.format(np.mean(vmae),np.mean(vrmse),np.mean(vmape),np.mean(vsmape)))\n",
        "    log = 'std:\\t{:.4f}\\t{:.4f}\\t{:.4f}\\t{:.4f}'\n",
        "    print(log.format(np.std(vmae),np.std(vrmse),np.std(vmape),np.std(vsmape)))\n",
        "    print('\\n\\n')\n",
        "    #test data\n",
        "    print('test|horizon\\tMAE-mean\\tRMSE-mean\\tMAPE-mean\\tMAE-std\\tRMSE-std\\tMAPE-std')\n",
        "    \n",
        "    for i in [2,5,11]:\n",
        "        log = '{:d}\\t{:.4f}\\t{:.4f}\\t{:.4f}\\t{:.4f}\\t{:.4f}\\t{:.4f}\\t{:.4f}\\t{:.4f}'\n",
        "        print(log.format(i+1, amae[i], armse[i], amape[i], asmape[i], smae[i], srmse[i], s_mape[i], ssmape[i]))\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "toc_visible": true,
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}